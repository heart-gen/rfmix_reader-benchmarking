{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 160em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################################\n",
    "#\n",
    "#   Load Libraries\n",
    "#\n",
    "###########################################################################################\n",
    "\n",
    "# IPython utilities\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import session_info\n",
    "from pyhere import here\n",
    "from pathlib import Path\n",
    "\n",
    "# Project specific\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import ast\n",
    "\n",
    "# Numerical and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#########################################################################################\n",
    "#\n",
    "#   Set Screen and Display Attributes\n",
    "#\n",
    "########################################################################################\n",
    "\n",
    "# Display all outputs from a cell (not just the last one)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Set notebook container width to full\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Set output window height to allow scrolling of long outputs\n",
    "display(HTML(\"<style>div.output_scroll { height: 160em; }</style>\"))\n",
    "\n",
    "# Set maximum rows, columns, and display width\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/p32505/users/manuel/rfmix_reader-benchmarking\n"
     ]
    }
   ],
   "source": [
    "print(here())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Skeleton script: compute global ancestry proportions per sample from per-chromosome\n",
    "# local-ancestry VCF-like files where each sample field is encoded as ancestry counts\n",
    "# separated by '|', e.g. for ancestries [CEU,YRI] a field looks like \"0|2\".\n",
    "\n",
    "# Features:\n",
    "# - Reads gzipped VCF files\n",
    "# - Parses header to get sample names and (optionally) ancestry labels from '##Ancestries'\n",
    "# - Two weighting modes: site-count (default) or interval-weighted (--weight)\n",
    "# - Basic CLI with argparse\n",
    "\n",
    "# Notes / assumptions:\n",
    "# - Each sample column contains a single token (or FORMAT-like token where the first\n",
    "#   subfield is the ancestry string). We take `field.split(':')[0]` to be safe.\n",
    "# - The ancestry token is a '|' separated list of integers, one per ancestry.\n",
    "# - The script works per input file (e.g. one chromosome file); you can pass multiple\n",
    "#   files and results will be summed across them.\n",
    "# - This is a skeleton; for very large datasets use cyvcf2/pysam and more memory-efficient\n",
    "#   approaches.\n",
    "\n",
    "# Requirements: Python 3.8+, standard library only.\n",
    "# \"\"\"\n",
    "\n",
    "# def open_vcf_gz(path: str):\n",
    "#     if path.endswith('.gz'):\n",
    "#         return gzip.open(path, 'rt')\n",
    "#     return open(path, 'r')\n",
    "\n",
    "\n",
    "# def parse_header_and_ancestries(f) -> Tuple[List[str], List[str]]:\n",
    "#     \"\"\"Read header lines and return (sample_names, ancestries).\n",
    "#     Assumes header lines start with '##' and the column header starts with '#CHROM'.\n",
    "#     If a '##Ancestries:' line exists we try to parse it (JSON-like). If not found\n",
    "#     caller must provide ancestries via CLI.\n",
    "#     \"\"\"\n",
    "#     ancestries = None\n",
    "#     sample_names = None\n",
    "#     # we will read from the beginning; if f is a file object already at start\n",
    "#     for line in f:\n",
    "#         if line.startswith('##'):\n",
    "#             if 'Ancestries' in line:\n",
    "#                 # try to extract a simple JSON-like list: [\"CEU\",\"YRI\"]\n",
    "#                 try:\n",
    "#                     import ast\n",
    "#                     right = line.split(':', 1)[1].strip()\n",
    "#                     ancestries = ast.literal_eval(right)\n",
    "#                 except Exception:\n",
    "#                     # leave ancestries as None and continue\n",
    "#                     ancestries = None\n",
    "#             continue\n",
    "#         if line.startswith('#CHROM'):\n",
    "#             parts = line.strip().split('\\t')\n",
    "#             sample_names = parts[9:]\n",
    "#             break\n",
    "#     return sample_names, ancestries\n",
    "\n",
    "\n",
    "# def parse_ancestry_token(token: str) -> List[int]:\n",
    "#     \"\"\"Parse a sample token like '0|2' or '0|2:otherinfo' -> [0,2]\n",
    "#     Returns list of ints (one per ancestry).\"\"\"\n",
    "#     if token is None or token == '.':\n",
    "#         return []\n",
    "#     token = token.split(':', 1)[0]\n",
    "#     parts = token.split('|')\n",
    "#     try:\n",
    "#         return [int(x) for x in parts]\n",
    "#     except ValueError:\n",
    "#         # if parsing fails, return empty to indicate missing data\n",
    "#         return []\n",
    "\n",
    "\n",
    "# def process_file(path: str,\n",
    "#                  ancestries: List[str],\n",
    "#                  weight_intervals: bool,\n",
    "#                  sample_sums: dict,\n",
    "#                  sample_total_weight: dict):\n",
    "#     \"\"\"Process one input file and update sample_sums (dict of sample->list(counts))\n",
    "#     and sample_total_weight (dict of sample->total_weight) in-place.\n",
    "\n",
    "#     If weight_intervals is False we just count sites (each site contributes its\n",
    "#     ancestry counts directly). If True, we weight ancestry calls by genomic\n",
    "#     interval to the next observed site (simple streaming approach).\n",
    "#     \"\"\"\n",
    "#     num_ances = len(ancestries)\n",
    "\n",
    "#     with open_maybe_gz(path) as fh:\n",
    "#         # parse header\n",
    "#         sample_names, file_ancestries = parse_header_and_ancestries(fh)\n",
    "#         if sample_names is None:\n",
    "#             raise RuntimeError(f\"No #CHROM header found in {path}\")\n",
    "#         if file_ancestries:\n",
    "#             # If file encodes ancestry labels, prefer them (but we still accept\n",
    "#             # user-provided ancestries)\n",
    "#             if file_ancestries and file_ancestries != ancestries:\n",
    "#                 print(\"Warning: file ancestries differ from provided ancestries; using provided list.\", file=sys.stderr)\n",
    "\n",
    "#         # initialize per-sample running variables if not already present\n",
    "#         for s in sample_names:\n",
    "#             if s not in sample_sums:\n",
    "#                 sample_sums[s] = [0.0] * num_ances\n",
    "#                 sample_total_weight[s] = 0.0\n",
    "\n",
    "#         prev_pos = None\n",
    "#         # store previous per-sample counts for interval weighting\n",
    "#         prev_counts = {s: None for s in sample_names}\n",
    "\n",
    "#         # iterate lines from current file pointer\n",
    "#         for line in fh:\n",
    "#             if line.startswith('#'):\n",
    "#                 continue\n",
    "#             parts = line.rstrip('\\n').split('\\t')\n",
    "#             chrom = parts[0]\n",
    "#             pos = int(parts[1])\n",
    "#             sample_fields = parts[9:]\n",
    "\n",
    "#             # parse ancestry tokens for this site for all samples\n",
    "#             parsed = [parse_ancestry_token(tok) for tok in sample_fields]\n",
    "\n",
    "#             # ensure parsed lengths match num_ances for each sample (pad with zeros)\n",
    "#             for i, arr in enumerate(parsed):\n",
    "#                 if not arr:\n",
    "#                     # missing -> treat as zeros\n",
    "#                     parsed[i] = [0] * num_ances\n",
    "#                 elif len(arr) < num_ances:\n",
    "#                     parsed[i] = arr + [0] * (num_ances - len(arr))\n",
    "\n",
    "#             if not weight_intervals:\n",
    "#                 # simple site-count: add counts directly, weight=1 per site\n",
    "#                 for sname, counts in zip(sample_names, parsed):\n",
    "#                     # each value in counts represents number of alleles for that ancestry\n",
    "#                     # so add them directly\n",
    "#                     for a in range(num_ances):\n",
    "#                         sample_sums[sname][a] += counts[a]\n",
    "#                     sample_total_weight[sname] += 2.0  # two alleles per site\n",
    "#                 continue\n",
    "\n",
    "#             # if weighting by intervals: if this is the first site, just store counts\n",
    "#             if prev_pos is None:\n",
    "#                 prev_pos = pos\n",
    "#                 for sname, counts in zip(sample_names, parsed):\n",
    "#                     prev_counts[sname] = counts\n",
    "#                 continue\n",
    "\n",
    "#             # interval length between prev_pos and current pos\n",
    "#             interval_len = pos - prev_pos\n",
    "#             if interval_len < 0:\n",
    "#                 interval_len = 0\n",
    "\n",
    "#             # add prev_counts weighted by interval_len\n",
    "#             for sname in sample_names:\n",
    "#                 counts = prev_counts[sname]\n",
    "#                 if counts is None:\n",
    "#                     continue\n",
    "#                 for a in range(num_ances):\n",
    "#                     sample_sums[sname][a] += counts[a] * interval_len\n",
    "#                 sample_total_weight[sname] += 2.0 * interval_len\n",
    "\n",
    "#             # shift: current becomes previous\n",
    "#             prev_pos = pos\n",
    "#             for sname, counts in zip(sample_names, parsed):\n",
    "#                 prev_counts[sname] = counts\n",
    "\n",
    "#         # after loop: we don't add an interval for the final site (alternatively\n",
    "#         # could extend to chromosome end if provided)\n",
    "\n",
    "\n",
    "# def compute_proportions(sample_sums: dict, sample_total_weight: dict, ancestries: List[str]):\n",
    "#     \"\"\"Return a dict sample -> dict(ancestry->proportion)\"\"\"\n",
    "#     out = {}\n",
    "#     for s, sums in sample_sums.items():\n",
    "#         total = sample_total_weight.get(s, 0.0)\n",
    "#         # fall back to counting-mode denominator if weighting wasn't used\n",
    "#         if total == 0.0:\n",
    "#             # assume sums were raw allele counts over sites: total alleles = sum(sums)\n",
    "#             total = sum(sums)\n",
    "#         props = {}\n",
    "#         for a, anc in enumerate(ancestries):\n",
    "#             props[anc] = (sums[a] / total) if total > 0 else 0.0\n",
    "#         out[s] = props\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def write_tsv(output_path: str, proportions: dict, ancestries: List[str]):\n",
    "#     with open(output_path, 'w') as out:\n",
    "#         header = ['Sample'] + ancestries\n",
    "#         out.write('\\t'.join(header) + '\\n')\n",
    "#         for s in sorted(proportions.keys()):\n",
    "#             row = [s] + [f\"{proportions[s][a]:.6f}\" for a in ancestries]\n",
    "#             out.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     p = argparse.ArgumentParser(description='Compute global ancestry proportions from local ancestry VCF-like files')\n",
    "#     p.add_argument('inputs', nargs='+', help='Input per-chromosome files (vcf or vcf.gz).')\n",
    "#     p.add_argument('--ancestries', nargs='+', help='Ordered list of ancestry labels (e.g. CEU YRI). If omitted, tries to read from file header.')\n",
    "#     p.add_argument('--weight', action='store_true', help='Weight calls by interval (pos[i+1]-pos[i]) instead of simple site counts.')\n",
    "#     p.add_argument('--out', default='global_ancestry.tsv', help='Output TSV file')\n",
    "#     args = p.parse_args()\n",
    "\n",
    "#     # prepare containers\n",
    "#     sample_sums = {}          # sample -> [sum_per_ancestry]\n",
    "#     sample_total_weight = {}  # sample -> total_weight (allele-units)\n",
    "\n",
    "#     # determine ancestries: prefer command-line, else try to read from first file\n",
    "#     ancestries = args.ancestries\n",
    "#     if ancestries is None:\n",
    "#         # try first file\n",
    "#         with open_maybe_gz(args.inputs[0]) as fh:\n",
    "#             _, file_ancestries = parse_header_and_ancestries(fh)\n",
    "#             if file_ancestries is None:\n",
    "#                 p.error('No ancestries provided and none found in file header. Pass --ancestries.')\n",
    "#             ancestries = file_ancestries\n",
    "\n",
    "#     print(f\"Using ancestries: {ancestries}\", file=sys.stderr)\n",
    "\n",
    "#     # process each file\n",
    "#     for path in args.inputs:\n",
    "#         print(f\"Processing {path} ...\", file=sys.stderr)\n",
    "#         process_file(path, ancestries, args.weight, sample_sums, sample_total_weight)\n",
    "\n",
    "#     # compute proportions\n",
    "#     proportions = compute_proportions(sample_sums, sample_total_weight, ancestries)\n",
    "\n",
    "#     # write output\n",
    "#     write_tsv(args.out, proportions, ancestries)\n",
    "#     print(f\"Wrote global ancestry table to {args.out}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inspecting: chr1.vcf.gz ===\n",
      "Samples: ['Sample_8', 'Sample_9', 'Sample_10', 'Sample_11', 'Sample_12', 'Sample_13', 'Sample_14', 'Sample_15', 'Sample_16', 'Sample_17', 'Sample_18', 'Sample_19', 'Sample_20', 'Sample_21', 'Sample_22', 'Sample_23', 'Sample_24', 'Sample_25', 'Sample_26', 'Sample_27', 'Sample_28', 'Sample_29', 'Sample_30', 'Sample_31', 'Sample_32', 'Sample_33', 'Sample_34', 'Sample_35', 'Sample_36', 'Sample_37', 'Sample_38', 'Sample_39', 'Sample_40', 'Sample_41', 'Sample_42', 'Sample_43', 'Sample_44', 'Sample_45', 'Sample_46', 'Sample_47', 'Sample_48', 'Sample_49', 'Sample_50', 'Sample_51', 'Sample_52', 'Sample_53', 'Sample_54', 'Sample_55', 'Sample_56', 'Sample_57', 'Sample_58', 'Sample_59', 'Sample_60', 'Sample_61', 'Sample_62', 'Sample_63', 'Sample_64', 'Sample_65', 'Sample_66', 'Sample_67', 'Sample_68', 'Sample_69', 'Sample_70', 'Sample_71', 'Sample_72', 'Sample_73', 'Sample_74', 'Sample_75', 'Sample_76', 'Sample_77', 'Sample_78', 'Sample_79', 'Sample_80', 'Sample_81', 'Sample_82', 'Sample_83', 'Sample_84', 'Sample_85', 'Sample_86', 'Sample_87', 'Sample_88', 'Sample_89', 'Sample_90', 'Sample_91', 'Sample_92', 'Sample_93', 'Sample_94', 'Sample_95', 'Sample_96', 'Sample_97', 'Sample_98', 'Sample_99', 'Sample_100', 'Sample_101', 'Sample_102', 'Sample_103', 'Sample_104', 'Sample_105', 'Sample_106', 'Sample_107', 'Sample_108', 'Sample_109', 'Sample_110', 'Sample_111', 'Sample_112', 'Sample_113', 'Sample_114', 'Sample_115', 'Sample_116', 'Sample_117', 'Sample_118', 'Sample_119', 'Sample_120', 'Sample_121', 'Sample_122', 'Sample_123', 'Sample_124', 'Sample_125', 'Sample_126', 'Sample_127', 'Sample_128', 'Sample_129', 'Sample_130', 'Sample_131', 'Sample_132', 'Sample_133', 'Sample_134', 'Sample_135', 'Sample_136', 'Sample_137', 'Sample_138', 'Sample_139', 'Sample_140', 'Sample_141', 'Sample_142', 'Sample_143', 'Sample_144', 'Sample_145', 'Sample_146', 'Sample_147', 'Sample_148', 'Sample_149', 'Sample_150', 'Sample_151', 'Sample_152', 'Sample_153', 'Sample_154', 'Sample_155', 'Sample_156', 'Sample_157', 'Sample_158', 'Sample_159', 'Sample_160', 'Sample_161', 'Sample_162', 'Sample_163', 'Sample_164', 'Sample_165', 'Sample_166', 'Sample_167', 'Sample_168', 'Sample_169', 'Sample_170', 'Sample_171', 'Sample_172', 'Sample_173', 'Sample_174', 'Sample_175', 'Sample_176', 'Sample_177', 'Sample_178', 'Sample_179', 'Sample_180', 'Sample_181', 'Sample_182', 'Sample_183', 'Sample_184', 'Sample_185', 'Sample_186', 'Sample_187', 'Sample_188', 'Sample_189', 'Sample_190', 'Sample_191', 'Sample_192', 'Sample_193', 'Sample_194', 'Sample_195', 'Sample_196', 'Sample_197', 'Sample_198', 'Sample_199', 'Sample_200', 'Sample_201', 'Sample_202', 'Sample_203', 'Sample_204', 'Sample_205', 'Sample_206', 'Sample_207', 'Sample_208', 'Sample_209', 'Sample_210', 'Sample_211', 'Sample_212', 'Sample_213', 'Sample_214', 'Sample_215', 'Sample_216', 'Sample_217', 'Sample_218', 'Sample_219', 'Sample_220', 'Sample_221', 'Sample_222', 'Sample_223', 'Sample_224', 'Sample_225', 'Sample_226', 'Sample_227', 'Sample_228', 'Sample_229', 'Sample_230', 'Sample_231', 'Sample_232', 'Sample_233', 'Sample_234', 'Sample_235', 'Sample_236', 'Sample_237', 'Sample_238', 'Sample_239', 'Sample_240', 'Sample_241', 'Sample_242', 'Sample_243', 'Sample_244', 'Sample_245', 'Sample_246', 'Sample_247', 'Sample_248', 'Sample_249', 'Sample_250', 'Sample_251', 'Sample_252', 'Sample_253', 'Sample_254', 'Sample_255', 'Sample_256', 'Sample_257', 'Sample_258', 'Sample_259', 'Sample_260', 'Sample_261', 'Sample_262', 'Sample_263', 'Sample_264', 'Sample_265', 'Sample_266', 'Sample_267', 'Sample_268', 'Sample_269', 'Sample_270', 'Sample_271', 'Sample_272', 'Sample_273', 'Sample_274', 'Sample_275', 'Sample_276', 'Sample_277', 'Sample_278', 'Sample_279', 'Sample_280', 'Sample_281', 'Sample_282', 'Sample_283', 'Sample_284', 'Sample_285', 'Sample_286', 'Sample_287', 'Sample_288', 'Sample_289', 'Sample_290', 'Sample_291', 'Sample_292', 'Sample_293', 'Sample_294', 'Sample_295', 'Sample_296', 'Sample_297', 'Sample_298', 'Sample_299', 'Sample_300', 'Sample_301', 'Sample_302', 'Sample_303', 'Sample_304', 'Sample_305', 'Sample_306', 'Sample_307', 'Sample_308', 'Sample_309', 'Sample_310', 'Sample_311', 'Sample_312', 'Sample_313', 'Sample_314', 'Sample_315', 'Sample_316', 'Sample_317', 'Sample_318', 'Sample_319', 'Sample_320', 'Sample_321', 'Sample_322', 'Sample_323', 'Sample_324', 'Sample_325', 'Sample_326', 'Sample_327', 'Sample_328', 'Sample_329', 'Sample_330', 'Sample_331', 'Sample_332', 'Sample_333', 'Sample_334', 'Sample_335', 'Sample_336', 'Sample_337', 'Sample_338', 'Sample_339', 'Sample_340', 'Sample_341', 'Sample_342', 'Sample_343', 'Sample_344', 'Sample_345', 'Sample_346', 'Sample_347', 'Sample_348', 'Sample_349', 'Sample_350', 'Sample_351', 'Sample_352', 'Sample_353', 'Sample_354', 'Sample_355', 'Sample_356', 'Sample_357', 'Sample_358', 'Sample_359', 'Sample_360', 'Sample_361', 'Sample_362', 'Sample_363', 'Sample_364', 'Sample_365', 'Sample_366', 'Sample_367', 'Sample_368', 'Sample_369', 'Sample_370', 'Sample_371', 'Sample_372', 'Sample_373', 'Sample_374', 'Sample_375', 'Sample_376', 'Sample_377', 'Sample_378', 'Sample_379', 'Sample_380', 'Sample_381', 'Sample_382', 'Sample_383', 'Sample_384', 'Sample_385', 'Sample_386', 'Sample_387', 'Sample_388', 'Sample_389', 'Sample_390', 'Sample_391', 'Sample_392', 'Sample_393', 'Sample_394', 'Sample_395', 'Sample_396', 'Sample_397', 'Sample_398', 'Sample_399', 'Sample_400', 'Sample_401', 'Sample_402', 'Sample_403', 'Sample_404', 'Sample_405', 'Sample_406', 'Sample_407', 'Sample_408', 'Sample_409', 'Sample_410', 'Sample_411', 'Sample_412', 'Sample_413', 'Sample_414', 'Sample_415', 'Sample_416', 'Sample_417', 'Sample_418', 'Sample_419', 'Sample_420', 'Sample_421', 'Sample_422', 'Sample_423', 'Sample_424', 'Sample_425', 'Sample_426', 'Sample_427', 'Sample_428', 'Sample_429', 'Sample_430', 'Sample_431', 'Sample_432', 'Sample_433', 'Sample_434', 'Sample_435', 'Sample_436', 'Sample_437', 'Sample_438', 'Sample_439', 'Sample_440', 'Sample_441', 'Sample_442', 'Sample_443', 'Sample_444', 'Sample_445', 'Sample_446', 'Sample_447', 'Sample_448', 'Sample_449', 'Sample_450', 'Sample_451', 'Sample_452', 'Sample_453', 'Sample_454', 'Sample_455', 'Sample_456', 'Sample_457', 'Sample_458', 'Sample_459', 'Sample_460', 'Sample_461', 'Sample_462', 'Sample_463', 'Sample_464', 'Sample_465', 'Sample_466', 'Sample_467', 'Sample_468', 'Sample_469', 'Sample_470', 'Sample_471', 'Sample_472', 'Sample_473', 'Sample_474', 'Sample_475', 'Sample_476', 'Sample_477', 'Sample_478', 'Sample_479', 'Sample_480', 'Sample_481', 'Sample_482', 'Sample_483', 'Sample_484', 'Sample_485', 'Sample_486', 'Sample_487', 'Sample_488', 'Sample_489', 'Sample_490', 'Sample_491', 'Sample_492', 'Sample_493', 'Sample_494', 'Sample_495', 'Sample_496', 'Sample_497', 'Sample_498', 'Sample_499', 'Sample_500']\n",
      "Ancestries: ['CEU', 'YRI']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSamples:\u001b[39m\u001b[33m\"\u001b[39m, samples)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAncestries:\u001b[39m\u001b[33m\"\u001b[39m, ancestries)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m df = \u001b[43mload_vcf_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvcf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head(), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# prints first few rows\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mload_vcf_to_df\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m header_line \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo #CHROM header found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader_line\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/projects/p32505/opt/env/AI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/projects/p32505/opt/env/AI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/projects/p32505/opt/env/AI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/projects/p32505/opt/env/AI_env/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/projects/p32505/opt/env/AI_env/lib/python3.12/gzip.py:346\u001b[39m, in \u001b[36mGzipFile.closed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mpeek() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer.peek(n)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def open_vcf_gz(path: str):\n",
    "    \"\"\"Open a plain text or gzipped file for reading.\"\"\"\n",
    "    return gzip.open(path, 'rt') if str(path).endswith('.gz') else open(path, 'r')\n",
    "\n",
    "def parse_header_and_ancestries(path) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Extract sample names and ancestries from the VCF header.\"\"\"\n",
    "    ancestries = None\n",
    "    sample_names = None\n",
    "    with open_maybe_gz(path) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('##'):\n",
    "                if 'Ancestries' in line:\n",
    "                    try:\n",
    "                        ancestries = ast.literal_eval(line.split(':', 1)[1].strip())\n",
    "                    except Exception:\n",
    "                        ancestries = None\n",
    "                continue\n",
    "            if line.startswith('#CHROM'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                sample_names = parts[9:]\n",
    "                break\n",
    "    return sample_names, ancestries\n",
    "\n",
    "def load_vcf_to_df(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load VCF-like file into a pandas DataFrame (metadata skipped).\"\"\"\n",
    "    with open_maybe_gz(path) as fh:\n",
    "        header_line = None\n",
    "        for line in fh:\n",
    "            if line.startswith('#CHROM'):\n",
    "                header_line = line.strip().split('\\t')\n",
    "                break\n",
    "        if header_line is None:\n",
    "            raise RuntimeError(f\"No #CHROM header found in {path}\")\n",
    "        df = pd.read_csv(fh, sep='\\t', header=None, names=header_line)\n",
    "    return df\n",
    "\n",
    "# Path to folder\n",
    "folder_path = Path(here(\"input/simulations/two_populations/ground_truth/_m\"))\n",
    "files = sorted(folder_path.glob(\"*.vcf.gz\"))\n",
    "\n",
    "# Raise clear error if no files\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No .vcf.gz files found in: {folder_path.resolve()}\")\n",
    "\n",
    "# Inspect files\n",
    "for vcf_path in files:\n",
    "    print(f\"\\n=== Inspecting: {vcf_path.name} ===\")\n",
    "    samples, ancestries = parse_header_and_ancestries(vcf_path)\n",
    "    print(\"Samples:\", samples)\n",
    "    print(\"Ancestries:\", ancestries)\n",
    "    \n",
    "    df = load_vcf_to_df(vcf_path)\n",
    "    print(df.head(), \"\\n\")  # prints first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda env",
   "language": "python",
   "name": "python-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
